{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f270da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33f77f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas\n",
    "#%pip install torch\n",
    "#%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e016e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is the datatset annotated by Jay\n",
    "data = pd.read_csv('final_dataset_06102022 2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3d1e6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(846, 96)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f79c79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "strs = [\"end his life\", \"end her life\", \"end my life\", \"end their lives\", \"ended his life\", \"ended her life\", \"ended my life\", \"ended their lives\", \"ending his life\", \"ending her life\", \"ending my life\", \"ending their lives\", \"ends his life\", \"ends her life\", \"ends my life\", \"ends their lives\", \"kill oneself\", \"kill himself\", \"kill herself\", \"kill theirselves\", \"kill myself\", \"killed oneself\", \"killed himself\", \"killed herself\", \"killed theirselves\", \"killed myself\", \"killing oneself\", \"killing himself\", \"killing herself\", \"killing theirselves\", \"killing myself\", \"kills oneself\", \"kills himself\", \"kills herself\", \"kills theirselves\", \"kills myself\", \"murder oneself\", \"murder himself\", \"murder herself\", \"murder theirselves\", \"murder myself\", \"murdered oneself\", \"murdered himself\", \"murdered herself\", \"murdered theirselves\", \"murdered myself\", \"murdering oneself\", \"murdering himself\", \"murdering herself\", \"murdering theirselves\", \"murdering myself\", \"murders oneself\", \"murders himself\", \"murders herself\", \"murders theirselves\", \"murders myself\", \"poison oneself\", \"poison himself\", \"poison herself\", \"poison theirselves\", \"poison myself\", \"poisoned oneself\", \"poisoned himself\", \"poisoned herself\", \"poisoned theirselves\", \"poisoned myself\", \"poisoning oneself\", \"poisoning himself\", \"poisoning herself\", \"poisoning theirselves\", \"poisoning myself\", \"poisons oneself\", \"poisons himself\", \"poisons herself\", \"poisons theirselves\", \"poisons myself\", \"drown oneself\", \"drown himself\", \"drown herself\", \"drown theirselves\", \"drown myself\", \"drowned oneself\", \"drowned himself\", \"drowned herself\", \"drowned theirselves\", \"drowned myself\", \"drowning oneself\", \"drowning himself\", \"drowning herself\", \"drowning theirselves\", \"drowning myself\", \"drowns oneself\", \"drowns himself\", \"drowns herself\", \"drowns theirselves\", \"drowns myself\", \"strangle oneself\", \"strangle himself\", \"strangle herself\", \"strangle theirselves\", \"strangle myself\", \"strangled oneself\",  \"strangled himself\", \"strangled herself\", \"strangled theirselves\", \"strangled myself\", \"strangling oneself\", \"strangling himself\", \"strangling herself\", \"strangling theirselves\", \"strangling myself\", \"strangles oneself\", \"strangles himself\", \"strangles herself\", \"strangles theirselves\", \"strangles myself\", \"suffocate oneself\", \"suffocate himself\", \"suffocate herself\", \"suffocate theirselves\", \"suffocate myself\", \"suffocated oneself\", \"suffocated himself\", \"suffocated herself\", \"suffocated theirselves\", \"suffocated myself\", \"suffocating oneself\", \"suffocating himself\", \"suffocating herself\", \"suffocating theirselves\", \"suffocating myself\", \"suffocates oneself\", \"suffocates himself\", \"suffocates herself\", \"suffocates theirselves\", \"suffocates myself\", \"jump to his death\", \"jump to her death\", \"jump to their death\", \"jump to my death\", \"jumped to his death\", \"jumped to her death\", \"jumped to their death\", \"jumped to my death\", \"jumping to his death\", \"jumping to her death\", \"jumping to their death\", \"jumping to my death\", \"jumps to his death\", \"jumps to her death\", \"jumps to their death\", \"jumps to my death\", \"hang himself\", \"hang oneself\", \"hang herself\", \"hang themselves\", \"hang myself\", \"hung himself\", \"hung oneself\", \"hung herself\", \"hung themselves\", \"hung myself\", \"hanging himself\", \"hanging oneself\", \"hanging herself\", \"hanging themselves\", \"hanging myself\", \"hangs himself\", \"hangs oneself\", \"hangs herself\", \"hangs themselves\", \"hangs myself\", \"hungs himself\", \"hungs oneself\", \"hungs herself\", \"hungs themselves\", \"hungs myself\", \"shoot oneself\", \"shoot himself\", \"shoot herself\", \"shoot myself\", \"shoot theirselves\", \"shot oneself\", \"shot himself\", \"shot herself\", \"shot myself\", \"shot theirselves\", \"shooting oneself\", \"shooting himself\", \"shooting herself\", \"shooting myself\", \"shooting theirselves\", \"shoots oneself\", \"shoots himself\", \"shoots herself\", \"shoots myself\", \"shoots theirselves\", \"shots oneself\", \"shots himself\", \"shots herself\", \"shots myself\", \"shots theirselves\", \"die by his hand\", \"die by his own hand\", \"die by her hand\", \"die by her own hand\", \"die by their hands\", \"die by their own hands\", \"die by my hand\", \"die by my own hand\", \"take his life\", \"take his own life\", \"take her life\", \"take her own life\", \"take their lives\", \"take their own lives\", \"take my life\", \"take my own life\", \"took his life\", \"took his own life\", \"took her life\", \"took her own life\", \"took their lives\", \"took their own lives\", \"took my life\", \"took my own life\", \"takes his life\", \"takes his own life\", \"takes her life\", \"takes her own life\", \"takes their lives\", \"takes their own lives\", \"takes my life\", \"takes my own life\", \"taking his life\", \"taking his own life\", \"taking her life\", \"taking her own life\", \"taking their lives\", \"taking their own lives\", \"taking my life\", \"taking my own life\",\"cut oneself\", \"cut himself\", \"cut herself\", \"cut theirselves\", \"cut myself\", \"cuts oneself\", \"cuts himself\", \"cuts herself\", \"cuts theirselves\", \"cuts myself\", \"cutting oneself\", \"cutting himself\", \"cutting herself\", \"cutting theirselves\", \"cutting myself\", \"stab oneself\", \"stab himself\", \"stab herself\", \"stab theirselves\", \"stab myself\", \"stabs oneself\", \"stabs himself\", \"stabs herself\", \"stabs theirselves\", \"stabs myself\", \"stabbing oneself\", \"stabbing himself\", \"stabbing herself\", \"stabbing theirselves\", \"stabbing myself\", \"stabbed oneself\", \"stabbed himself\", \"stabbed herself\", \"stabbed theirselves\", \"stabbed myself\", \"cut his throat\", \"cut her throat\", \"cut my throat\", \"cuts his throat\", \"cuts her throat\", \"cuts my throat\", \"cutting his throat\", \"cutting her throat\", \"cutting my throat\", \"cut his wrist\", \"cut her wrist\", \"cut my wrist\", \"cuts his wrist\", \"cuts her wrist\", \"cuts my wrist\", \"cutting his wrist\", \"cutting her wrist\", \"cutting my wrist\", \"slit his throat\", \"slit her throat\", \"slit my throat\", \"slits his throat\", \"slits her throat\", \"slits my throat\", \"slitting his throat\", \"slitting her throat\", \"slitting my throat\", \"slit his wrist\", \"slit her wrist\", \"slit my wrist\", \"slits his wrist\", \"slits her wrist\", \"slits my wrist\", \"slitting his wrist\", \"slitting her wrist\", \"slitting my wrist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73d8e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Yes'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3d9901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'^news[0-6]_'\n",
    "datacopy = data.copy()\n",
    "datacopy['Document name'] = data['Document name'].str.replace(pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7035294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 96)\n"
     ]
    }
   ],
   "source": [
    "data.head(3)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10d46be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "from transformers import BertTokenizer,BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8de51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer,util\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define your sentence transformer model using CLS pooling\n",
    "#model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "#word_embedding_model = models.Transformer(model_name)\n",
    "#print(word_embedding_model.get_word_embedding_dimension())\n",
    "#pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls')\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7441bcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "print(model.max_seq_length)\n",
    "#model.max_seq_length = 500\n",
    "print(model.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76a0770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FinancialLabel = \"Financial/Job Problem\"\n",
    "Legallabel = \"Legal Problem\"\n",
    "SchoolLabel = \"School or Academic Related Problem\"\n",
    "HealthLabel = \"Lack of Access to Health/Mental Health Care\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f19b2d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 96)\n",
      "(200, 96)\n",
      "Legal Problem :  39\n",
      "Financial/Job Problem :  13\n",
      "Lack of Access to Health/Mental Health Care :  7\n",
      "School or Academic Related Problem :  5\n",
      "\n",
      "Other non structural factors: \n",
      "Explicit Statement of Mental Health Symptoms or Diagnosis :  35\n",
      "Social/Relationship Problem :  30\n",
      "Physical Health Problem :  26\n",
      "Crisis plus-minus 2 weeks :  8\n",
      "Loss of Sense of Purpose or Hope:  11\n",
      "History of Child Abuse :  6\n",
      "Lack of Social Support :  6\n",
      "Death of Friend or Family Member :  2\n",
      "Alcohol Dependence :  2\n",
      "Other Substance Problem :  7\n",
      "Disengaged in Activities :  0\n",
      "Access to Means to Harm Self :  2\n",
      "Other Circumstance:  8\n",
      "Unspecified Circumstance:  56\n"
     ]
    }
   ],
   "source": [
    "relevant = data[data['Yes']==1]\n",
    "print(relevant.shape)\n",
    "print(data.shape)\n",
    "\n",
    "financial_problem = data[data[FinancialLabel] > 0]\n",
    "school_problem = data[data[SchoolLabel] > 0]\n",
    "legal_problem = data[data[Legallabel] > 0]\n",
    "health_problem = data[data[HealthLabel] > 0]\n",
    "unspecified_circumstance = data[data['Unspecified Circumstance'] > 0]\n",
    "\n",
    "print(\"Legal Problem : \", legal_problem.shape[0])\n",
    "print(\"Financial/Job Problem : \", financial_problem.shape[0])\n",
    "print(\"Lack of Access to Health/Mental Health Care : \", health_problem.shape[0])\n",
    "print(\"School or Academic Related Problem : \", school_problem.shape[0])\n",
    "print(\"\")\n",
    "print(\"Other non structural factors: \")\n",
    "\n",
    "print(\"Explicit Statement of Mental Health Symptoms or Diagnosis : \", data[data['Explicit Statement of Mental Health Symptoms or Diagnosis'] > 0].shape[0])\n",
    "print(\"Social/Relationship Problem : \", data[data['Social/Relationship Problem'] > 0].shape[0])\n",
    "print(\"Physical Health Problem : \", data[data['Physical Health Problem'] > 0].shape[0])\n",
    "print(\"Crisis plus-minus 2 weeks : \", data[data['Crisis plus-minus 2 weeks'] > 0].shape[0])\n",
    "print(\"Loss of Sense of Purpose or Hope: \", data[data['Loss of Sense of Purpose or Hope'] > 0].shape[0])\n",
    "print(\"History of Child Abuse : \", data[data['History of Child Abuse'] > 0].shape[0])\n",
    "print(\"Lack of Social Support : \", data[data['Lack of Social Support'] > 0].shape[0])\n",
    "print(\"Death of Friend or Family Member : \", data[data['Death of Friend or Family Member'] > 0].shape[0])\n",
    "print(\"Alcohol Dependence : \", data[data['Alcohol Dependence'] > 0].shape[0])\n",
    "print(\"Other Substance Problem : \", data[data['Other Substance Problem'] > 0].shape[0])\n",
    "print(\"Disengaged in Activities : \", data[data['Disengaged in Activities'] > 0].shape[0])\n",
    "print(\"Access to Means to Harm Self : \", data[data['Access to Means to Harm Self'] > 0].shape[0])\n",
    "print(\"Other Circumstance: \", data[data['Other Circumstance'] > 0].shape[0])\n",
    "print(\"Unspecified Circumstance: \", data[data['Unspecified Circumstance'] > 0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4f4ebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count:  1285.6\n"
     ]
    }
   ],
   "source": [
    "school_problem_df = pd.DataFrame(school_problem, columns=['Document group', 'Document name'])\n",
    "arrayoftexts = []\n",
    "wordcount = 0.0\n",
    "for documentname in school_problem_df['Document name']:\n",
    "    #print(documentname)\n",
    "    with open(\"articles/\"+ documentname + \".txt\") as f:\n",
    "        text = f.readlines()\n",
    "        arrayoftexts.append(text[0])\n",
    "        wordcount += len(text[0].split())\n",
    "avgwordcount = wordcount/len(arrayoftexts)\n",
    "\n",
    "school_problem_df['Text'] = arrayoftexts\n",
    "\n",
    "school_problem_embeddings = []\n",
    "\n",
    "for text in school_problem_df['Text']:\n",
    "    embedding = model.encode(text)\n",
    "    school_problem_embeddings.append(embedding)\n",
    "\n",
    "school_problem_averaged_embedding = np.mean(school_problem_embeddings, axis = 0)\n",
    "print(\"Average word count: \", avgwordcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0b77961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count:  1058.4615384615386\n"
     ]
    }
   ],
   "source": [
    "legal_problem_df = pd.DataFrame(legal_problem, columns=['Document group', 'Document name'])\n",
    "arrayoftexts = []\n",
    "wordcount = 0.0\n",
    "for documentname in legal_problem_df['Document name']:\n",
    "    #print(documentname)\n",
    "    with open(\"articles/\"+ documentname + \".txt\") as f:\n",
    "        text = f.readlines()\n",
    "        arrayoftexts.append(text[0])\n",
    "        wordcount += len(text[0].split())\n",
    "avgwordcount = wordcount/len(arrayoftexts)\n",
    "\n",
    "legal_problem_df['Text'] = arrayoftexts\n",
    "\n",
    "legal_problem_embeddings = []\n",
    "\n",
    "for text in legal_problem_df['Text']:\n",
    "    embedding = model.encode(text)\n",
    "    legal_problem_embeddings.append(embedding)\n",
    "\n",
    "legal_problem_averaged_embedding = np.mean(legal_problem_embeddings, axis = 0)\n",
    "print(\"Average word count: \", avgwordcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8efd7644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count:  1497.6153846153845\n"
     ]
    }
   ],
   "source": [
    "financial_problem_df = pd.DataFrame(financial_problem, columns=['Document group', 'Document name'])\n",
    "arrayoftexts = []\n",
    "wordcount = 0.0\n",
    "for documentname in financial_problem_df['Document name']:\n",
    "    #print(documentname)\n",
    "    with open(\"articles/\"+ documentname + \".txt\") as f:\n",
    "        text = f.readlines()\n",
    "        arrayoftexts.append(text[0])\n",
    "        wordcount += len(text[0].split())\n",
    "avgwordcount = wordcount/len(arrayoftexts)\n",
    "\n",
    "financial_problem_df['Text'] = arrayoftexts\n",
    "\n",
    "financial_problem_embeddings = []\n",
    "\n",
    "for text in financial_problem_df['Text']:\n",
    "    embedding = model.encode(text)\n",
    "    financial_problem_embeddings.append(embedding)\n",
    "\n",
    "financial_problem_averaged_embedding = np.mean(financial_problem_embeddings, axis = 0)\n",
    "print(\"Average word count: \", avgwordcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a83127c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count:  949.5714285714286\n"
     ]
    }
   ],
   "source": [
    "health_problem_df = pd.DataFrame(health_problem, columns=['Document group', 'Document name'])\n",
    "arrayoftexts = []\n",
    "wordcount = 0.0\n",
    "for documentname in health_problem_df['Document name']:\n",
    "    #print(documentname)\n",
    "    with open(\"articles/\"+ documentname + \".txt\") as f:\n",
    "        text = f.readlines()\n",
    "        arrayoftexts.append(text[0])\n",
    "        wordcount += len(text[0].split())\n",
    "avgwordcount = wordcount/len(arrayoftexts)\n",
    "\n",
    "health_problem_df['Text'] = arrayoftexts\n",
    "\n",
    "health_problem_embeddings = []\n",
    "\n",
    "for text in health_problem_df['Text']:\n",
    "    embedding = model.encode(text)\n",
    "    health_problem_embeddings.append(embedding)\n",
    "\n",
    "health_problem_averaged_embedding = np.mean(health_problem_embeddings, axis = 0)\n",
    "print(\"Average word count: \", avgwordcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d49771fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "texts = [FinancialLabel, Legallabel, SchoolLabel, HealthLabel] \n",
    "embeddingsize = len(financial_problem_averaged_embedding)\n",
    "header = [i for i in range(embeddingsize)]\n",
    "header.insert(0, \"Structural Violence Factor\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdad1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_final = [financial_problem_averaged_embedding.tolist(), legal_problem_averaged_embedding.tolist(), school_problem_averaged_embedding.tolist(), health_problem_averaged_embedding.tolist()]\n",
    "for i in range(len(embeddings_final)):\n",
    "    embeddings_final[i].insert(0, texts[i])\n",
    "\n",
    "with open('200 labelled embeddings only first 384 words.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(embeddings_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "124557eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19386, 3)\n"
     ]
    }
   ],
   "source": [
    "# Dataset from ben Horne\n",
    "bendata = pd.read_csv('Transfer/article_matches.csv')\n",
    "bendata.head(3)\n",
    "print(bendata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a62733e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count:  1126.8050869318474\n"
     ]
    }
   ],
   "source": [
    "bendata = bendata[~bendata['content'].isna()]\n",
    "suicide_Articles_from_ben_data = bendata['content'].tolist()\n",
    "\n",
    "wordcount = 0.0\n",
    "for art in suicide_Articles_from_ben_data:\n",
    "    wordcount += len(art.split())\n",
    "\n",
    "print(\"Average word count: \", wordcount/len(suicide_Articles_from_ben_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1430780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19383, 3)\n"
     ]
    }
   ],
   "source": [
    "print(bendata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48fa5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Load sentences & embeddings from disc\n",
    "with open('ben_embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_bendata_sentences = stored_data['sentences']\n",
    "    stored_bendata_embeddings = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07b1dfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19383\n",
      "19383\n"
     ]
    }
   ],
   "source": [
    "print(len(stored_bendata_sentences))\n",
    "print(len(stored_bendata_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76f63d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_bendata_financial = util.cos_sim(stored_bendata_embeddings, financial_problem_averaged_embedding)\n",
    "cos_sim_bendata_legal = util.cos_sim(stored_bendata_embeddings, legal_problem_averaged_embedding)\n",
    "cos_sim_bendata_school = util.cos_sim(stored_bendata_embeddings, school_problem_averaged_embedding)\n",
    "cos_sim_bendata_health = util.cos_sim(stored_bendata_embeddings, health_problem_averaged_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b344fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max similarity for financial problem:  tensor(0.7427)\n",
      "Max similarity for legal problem:  tensor(0.7710)\n",
      "Max similarity for school problem:  tensor(0.8391)\n",
      "Max similarity for health problem:  tensor(0.8231)\n"
     ]
    }
   ],
   "source": [
    "print(\"Max similarity for financial problem: \", torch.max(cos_sim_bendata_financial))\n",
    "print(\"Max similarity for legal problem: \", torch.max(cos_sim_bendata_legal))\n",
    "print(\"Max similarity for school problem: \", torch.max(cos_sim_bendata_school))\n",
    "print(\"Max similarity for health problem: \", torch.max(cos_sim_bendata_health))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c22a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(suicide_Articles_from_ben_data[torch.argmax(cos_sim_bendata_financial).item()])\n",
    "# print(\"next\")\n",
    "# print(suicide_Articles_from_ben_data[torch.argmax(cos_sim_bendata_legal).item()])\n",
    "# print(\"next\")\n",
    "# print(suicide_Articles_from_ben_data[torch.argmax(cos_sim_bendata_school).item()])\n",
    "# print(\"next\")\n",
    "# print(suicide_Articles_from_ben_data[torch.argmax(cos_sim_bendata_health).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75c6afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_embeddings = financial_problem_embeddings + legal_problem_embeddings + school_problem_embeddings + health_problem_embeddings\n",
    "\n",
    "# labels = []\n",
    "# for i in range(len(financial_problem_embeddings)):\n",
    "#     labels.append(FinancialLabel)\n",
    "# for i in range(len(legal_problem_embeddings)):\n",
    "#     labels.append(Legallabel)\n",
    "# for i in range(len(school_problem_embeddings)):\n",
    "#     labels.append(SchoolLabel)\n",
    "# for i in range(len(health_problem_embeddings)):\n",
    "#     labels.append(HealthLabel)\n",
    "\n",
    "# print(np.array(all_embeddings).shape)\n",
    "# print(np.array(labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b80c7b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 768)\n",
      "(19383, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minijain/Library/Python/3.9/lib/python/site-packages/sentence_transformers/util.py:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  b = torch.tensor(b)\n"
     ]
    }
   ],
   "source": [
    "all_avg_embeddings = []\n",
    "all_avg_embeddings.append(financial_problem_averaged_embedding)\n",
    "all_avg_embeddings.append(legal_problem_averaged_embedding)\n",
    "all_avg_embeddings.append(school_problem_averaged_embedding)\n",
    "all_avg_embeddings.append(health_problem_averaged_embedding)\n",
    "print(np.array(all_avg_embeddings).shape)\n",
    "\n",
    "cos_sim_bendata_avg_embeddings = util.cos_sim(stored_bendata_embeddings, all_avg_embeddings)\n",
    "print(np.array(cos_sim_bendata_avg_embeddings).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e572a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MAJORITY VOTING BASED CLASSIFICATION\n",
    "\n",
    "# #Find the pairs with the highest cosine similarity scores\n",
    "# # IMPORTANT - COSINE SIMILARITY FUNCTION SHOULD BE BASSED CORRECT EMBEDDINGS ARRAY\n",
    "# cosine_scores = util.cos_sim(stored_bendata_embeddings, all_embeddings)\n",
    "# print(len(cosine_scores))\n",
    "# pairs = []\n",
    "# for i in range(len(cosine_scores)):\n",
    "#     for j in range(len(cosine_scores[0])):\n",
    "#         pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "\n",
    "# #Sort scores in decreasing order\n",
    "# pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "# print(np.array(pairs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c7de19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# topk = 5\n",
    "# threshhold = 0.5\n",
    "# with open('ben_dataset_all_class_scores_individual_embeddings.csv', 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"Id\", \"Article\", FinancialLabel, Legallabel, SchoolLabel, HealthLabel])\n",
    "#     for i in range(len(cosine_scores)):\n",
    "#         articlestats = cosine_scores[i]\n",
    "#         financial_score = torch.max(articlestats[np.argwhere(np.array(labels) == FinancialLabel).flatten()]).item()\n",
    "#         legal_score = torch.max(articlestats[np.argwhere(np.array(labels) == Legallabel).flatten()]).item()\n",
    "#         school_score = torch.max(articlestats[np.argwhere(np.array(labels) == SchoolLabel).flatten()]).item()\n",
    "#         health_score = torch.max(articlestats[np.argwhere(np.array(labels) == HealthLabel).flatten()]).item()        \n",
    "#         arr = [i+1, stored_bendata_sentences[i],financial_score, legal_score, school_score, health_score]\n",
    "#         writer.writerow(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9093151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19383, 4)\n",
      "(77532,)\n"
     ]
    }
   ],
   "source": [
    "#AVERAGING BASED CLASSIFICATION\n",
    "\n",
    "#Find the pairs with the highest cosine similarity scores\n",
    "# IMPORTANT - COSINE SIMILARITY FUNCTION SHOULD BE BASSED CORRECT EMBEDDINGS ARRAY\n",
    "cosine_scores = util.cos_sim(stored_bendata_embeddings, all_avg_embeddings)\n",
    "print(np.array(cosine_scores).shape)\n",
    "pairs = []\n",
    "for i in range(len(cosine_scores)):\n",
    "    for j in range(len(cosine_scores[0])):\n",
    "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "\n",
    "#Sort scores in decreasing order\n",
    "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "print(np.array(pairs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "465531e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [FinancialLabel, Legallabel, SchoolLabel, HealthLabel] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d1c128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headerrow = [\"Score\",\"Label\", \"Article\"]\n",
    "# rows = []\n",
    "# for pair in pairs[0:20]:\n",
    "#     i, j = pair['index']\n",
    "#     rows.append([pair['score'].item(), texts[j], stored_bendata_sentences[i]])\n",
    "\n",
    "# with open('ben_dataset_analysis_top20_highest_matches_avg_embeddings.csv', 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(headerrow)\n",
    "#     writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5249a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores = util.cos_sim(stored_bendata_embeddings, all_avg_embeddings)\n",
    "with open('ben_dataset_all_class_scores_embeddings_using_first_384_words.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Id\", \"Article\", FinancialLabel, Legallabel, SchoolLabel, HealthLabel])\n",
    "    for i in range(len(cosine_scores)):\n",
    "        articlestats = cosine_scores[i]\n",
    "        arr = [i+1, stored_bendata_sentences[i],articlestats[0].item(), articlestats[1].item(), articlestats[2].item(), articlestats[3].item()]\n",
    "        writer.writerow(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5da17c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
